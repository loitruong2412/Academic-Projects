{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "## download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "## download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "# remove special characters\n",
    "specialChar = '[=@_!#$%^&*()<>?/\\|}{~:`]'\n",
    "for string in specialChar:\n",
    "    comments['comment'] = comments['comment'].apply(lambda x: x.replace(string, \" \"))\n",
    "\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.split())\n",
    "s = ' '\n",
    "comments['comment'] = comments['comment'].apply(lambda x: s.join(x))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279                           Iraq is not good USA is bad\n",
       "2702703    fuck off you little asshole. If you want to ta...\n",
       "4632658          i have a dick, its bigger than yours hahaha\n",
       "6545332    renault you sad little bpy for driving a renau...\n",
       "6545351    renault you sad little bo for driving a renaul...\n",
       "7977970    34, 30 Nov 2004 UTC Because you like to accuse...\n",
       "8359431    You are not worth the effort. You are arguing ...\n",
       "8724028    Yes, complain to your rabbi and then go shoot ...\n",
       "8845700                     i am using the sandbox, ass wipe\n",
       "8845736    GOD DAMN GOD DAMN it fuckers, i am using the G...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.97     20422\n",
      "        True       0.91      0.56      0.69      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.93      0.77      0.83     23178\n",
      "weighted avg       0.94      0.94      0.93     23178\n",
      "\n",
      "[[20268   154]\n",
      " [ 1226  1530]]\n"
     ]
    }
   ],
   "source": [
    "# fit a simple text classifier\n",
    "# With FeatureUnion, only param: analyzer='word' and analyzer='char', AUC = 0.959\n",
    "# With CountVect + TfidfTransformer, AUC = 0.957\n",
    "# No FeatureUnion, 1 Vectorizer AUC = 0.958\n",
    "# With FeatureUnion, AUC = 0.958\n",
    "\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "# clf = Pipeline([\n",
    "    #('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    #('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LogisticRegression()),\n",
    "#])\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "#               'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "#               'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "#               'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "#               'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "#               'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "#               'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "#               'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "#               'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "#               'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "vectorizerW = TfidfVectorizer(analyzer='word')\n",
    "vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "print(classification_report(trueVals, predictedVals))\n",
    "print(confusion_matrix(trueVals,predictedVals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC model. Taking forever\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "svm = SVC()\n",
    "classifier = CalibratedClassifierCV(svm)\n",
    "vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "print(classification_report(trueVals, predictedVals))\n",
    "print(confusion_matrix(trueVals,predictedVals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:97: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# LinearDiscriminantAnalysis. Kernel died\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "clf = Pipeline([(\"features\", combined_features), \n",
    "                (\"trans\", FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\n",
    "                (\"clf\", classifier)])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "print(classification_report(trueVals, predictedVals))\n",
    "print(confusion_matrix(trueVals,predictedVals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:568: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.930\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.98      0.96     20422\n",
      "        True       0.78      0.64      0.70      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.87      0.81      0.83     23178\n",
      "weighted avg       0.93      0.94      0.93     23178\n",
      "\n",
      "[[19939   483]\n",
      " [  994  1762]]\n"
     ]
    }
   ],
   "source": [
    "# MLPClassifier. Also slow \n",
    "# AUC: 0.930\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = MLPClassifier()\n",
    "vectorizerW = TfidfVectorizer(analyzer='word')\n",
    "vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "print(classification_report(trueVals, predictedVals))\n",
    "print(confusion_matrix(trueVals,predictedVals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "# No FeatureUnion, 1 Vectorizer AUC = 0.894\n",
    "# With FeatureUnion, AUC = 0.874\n",
    "# With CountVectorizer + TfidfTransformer AUC = 0.904\n",
    "# With FeatureUnion, only param: analyzer='word' and analyzer='char', AUC = 0.868\n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "    classifier = RandomForestClassifier()\n",
    "    vectorizerW = TfidfVectorizer(analyzer='word')\n",
    "    vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "    combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "    clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "\n",
    "# vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "# vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "# combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "# clf = Pipeline([('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "                # ('tfidf', TfidfTransformer(norm = 'l2')), \n",
    "                # (\"clf\", classifier)])\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    # clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "    # auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "    # trueVals = test_comments['attack']\n",
    "    # predictedVals = clf.predict(test_comments['comment'])\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96     20387\n",
      "        True       0.85      0.55      0.67      2786\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.90      0.77      0.82     23173\n",
      "weighted avg       0.93      0.93      0.93     23173\n",
      "\n",
      "[[20125   262]\n",
      " [ 1253  1533]]\n",
      "Test ROC AUC: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96     20458\n",
      "        True       0.85      0.54      0.66      2715\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.90      0.77      0.81     23173\n",
      "weighted avg       0.93      0.94      0.93     23173\n",
      "\n",
      "[[20200   258]\n",
      " [ 1238  1477]]\n",
      "Test ROC AUC: 0.940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96     20465\n",
      "        True       0.84      0.56      0.67      2708\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.89      0.77      0.82     23173\n",
      "weighted avg       0.93      0.94      0.93     23173\n",
      "\n",
      "[[20185   280]\n",
      " [ 1204  1504]]\n",
      "Test ROC AUC: 0.935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96     20486\n",
      "        True       0.84      0.54      0.66      2687\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.89      0.76      0.81     23173\n",
      "weighted avg       0.93      0.93      0.93     23173\n",
      "\n",
      "[[20209   277]\n",
      " [ 1234  1453]]\n",
      "Test ROC AUC: 0.931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.96     20478\n",
      "        True       0.85      0.54      0.66      2694\n",
      "\n",
      "    accuracy                           0.94     23172\n",
      "   macro avg       0.90      0.76      0.81     23172\n",
      "weighted avg       0.93      0.94      0.93     23172\n",
      "\n",
      "[[20228   250]\n",
      " [ 1250  1444]]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "# With FeatureUnion, only param: analyzer='word' and analyzer='char', AUC = 0.858\n",
    "# No FeatureUnion, 1 Vectorizer AUC = 0.837\n",
    "# With FeatureUnion, AUC = 0.858 \n",
    "# With CountVectorizer + TfidfTransformer AUC = 0.936\n",
    "\n",
    "\n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "    classifier = MultinomialNB()\n",
    "# vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "# vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "# combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "# clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "    clf = Pipeline([('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "                    ('tfidf', TfidfTransformer(norm = 'l2')), \n",
    "                    (\"clf\", classifier)])\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    # clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "    # auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "    # trueVals = test_comments['attack']\n",
    "    # predictedVals = clf.predict(test_comments['comment'])\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20387\n",
      "        True       0.94      0.38      0.54      2786\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.93      0.69      0.75     23173\n",
      "weighted avg       0.92      0.92      0.91     23173\n",
      "\n",
      "[[20320    67]\n",
      " [ 1726  1060]]\n",
      "Test ROC AUC: 0.941\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20458\n",
      "        True       0.94      0.37      0.53      2715\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.93      0.69      0.75     23173\n",
      "weighted avg       0.92      0.92      0.91     23173\n",
      "\n",
      "[[20390    68]\n",
      " [ 1699  1016]]\n",
      "Test ROC AUC: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20465\n",
      "        True       0.95      0.39      0.55      2708\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.94      0.69      0.76     23173\n",
      "weighted avg       0.93      0.93      0.91     23173\n",
      "\n",
      "[[20408    57]\n",
      " [ 1657  1051]]\n",
      "Test ROC AUC: 0.939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20486\n",
      "        True       0.92      0.38      0.54      2687\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.92      0.69      0.75     23173\n",
      "weighted avg       0.92      0.92      0.91     23173\n",
      "\n",
      "[[20391    95]\n",
      " [ 1657  1030]]\n",
      "Test ROC AUC: 0.934\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20478\n",
      "        True       0.94      0.38      0.54      2694\n",
      "\n",
      "    accuracy                           0.93     23172\n",
      "   macro avg       0.93      0.69      0.75     23172\n",
      "weighted avg       0.93      0.93      0.91     23172\n",
      "\n",
      "[[20415    63]\n",
      " [ 1671  1023]]\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier\n",
    "# With FeatureUnion, only param: analyzer='word' and analyzer='char', AUC = 0.939\n",
    "# With CountVectorizer + TfidfTransformer AUC = 0.944\n",
    "# With FeatureUnion, AUC = 0.939\n",
    "# No FeatureUnion, 1 Vectorizer AUC = 0.940 \n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "# train_comments = comments.query(\"split=='train'\")\n",
    "# test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "    classifier = SGDClassifier(loss='log')\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "# vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "# combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "# clf = Pipeline([(\"vect\", vectorizer), (\"clf\", classifier)])\n",
    "\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word')),\n",
    "                    ('tfidf', TfidfTransformer()), \n",
    "                    (\"clf\", classifier)])\n",
    "        \n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    # clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "    # auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "    # trueVals = test_comments['attack']\n",
    "    # predictedVals = clf.predict(test_comments['comment'])\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.99      0.96     20387\n",
      "        True       0.91      0.41      0.56      2786\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.92      0.70      0.76     23173\n",
      "weighted avg       0.92      0.92      0.91     23173\n",
      "\n",
      "[[20275   112]\n",
      " [ 1647  1139]]\n",
      "Test ROC AUC: 0.940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      1.00      0.96     20458\n",
      "        True       0.93      0.41      0.57      2715\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.93      0.70      0.76     23173\n",
      "weighted avg       0.93      0.93      0.91     23173\n",
      "\n",
      "[[20368    90]\n",
      " [ 1604  1111]]\n",
      "Test ROC AUC: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.99      0.96     20465\n",
      "        True       0.92      0.43      0.59      2708\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.92      0.71      0.78     23173\n",
      "weighted avg       0.93      0.93      0.92     23173\n",
      "\n",
      "[[20356   109]\n",
      " [ 1532  1176]]\n",
      "Test ROC AUC: 0.937\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      1.00      0.96     20486\n",
      "        True       0.91      0.40      0.56      2687\n",
      "\n",
      "    accuracy                           0.93     23173\n",
      "   macro avg       0.92      0.70      0.76     23173\n",
      "weighted avg       0.93      0.93      0.91     23173\n",
      "\n",
      "[[20384   102]\n",
      " [ 1601  1086]]\n",
      "Test ROC AUC: 0.932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      1.00      0.96     20478\n",
      "        True       0.92      0.41      0.57      2694\n",
      "\n",
      "    accuracy                           0.93     23172\n",
      "   macro avg       0.92      0.70      0.76     23172\n",
      "weighted avg       0.93      0.93      0.91     23172\n",
      "\n",
      "[[20382    96]\n",
      " [ 1592  1102]]\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier\n",
    "# With FeatureUnion, only param: analyzer='word' and analyzer='char'\n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "# train_comments = comments.query(\"split=='train'\")\n",
    "# test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "# x_train = train_comments['comment']\n",
    "# y_train = train_comments['attack']\n",
    "# x_test = test_comments['comment']\n",
    "# y_test = test_comments['attack']\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "    classifier = SGDClassifier(loss='log')\n",
    "    vectorizerW = TfidfVectorizer(analyzer='word')\n",
    "    vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "    combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "    clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "    #clf = Pipeline([('vect', CountVectorizer(analyzer='word')),\n",
    "                    #('tfidf', TfidfTransformer()), \n",
    "                    #(\"clf\", classifier)])\n",
    "        \n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    # clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "    # auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "    # trueVals = test_comments['attack']\n",
    "    # predictedVals = clf.predict(test_comments['comment'])\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 25.6min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 32.7min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed: 37.0min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 47.6min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 51.4min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 55.9min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 61.3min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 65.2min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 69.7min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 74.2min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 78.2min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 83.1min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 87.2min\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for SGDClassifier Take 1\n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "\n",
    "# X = np.array(comments['comment'])\n",
    "# y = np.array(comments['attack'])\n",
    "\n",
    "# for train_comments, test_comments in kfold.split(X):\n",
    "#     X_train, X_test = X[train_comments], X[test_comments]\n",
    "#     y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "X_train = train_comments['comment']\n",
    "y_train = train_comments['attack']\n",
    "X_test = test_comments['comment']\n",
    "y_test = test_comments['attack']\n",
    "\n",
    "classifier = SGDClassifier(loss='log')\n",
    "vectorizerW = TfidfVectorizer(analyzer='word')\n",
    "vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "param_grid = dict(\n",
    "            features__word__max_features=[2000,4000],\n",
    "            features__char__max_features=[2000,4000],\n",
    "            features__word__min_df=[2,3],\n",
    "            features__char__min_df=[2,3],\n",
    "            features__word__ngram_range=[(1,2), (1,3)],\n",
    "            features__char__ngram_range=[(3,3), (3,4), (4,4)],\n",
    "            #features__word__stop_words=['english', None],\n",
    "            features__word__lowercase=[True],\n",
    "            features__char__lowercase=[True],\n",
    "            clf__alpha=[1e-2, 1e-3]\n",
    "            )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=3,verbose=10, n_jobs=-1)\n",
    "if __name__ == \"__main__\":\n",
    "# fit on TRAINING data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                            #print(\"\\n\")\n",
    "    \n",
    "# Run the grid_search transforms+prediction with best parameters on test data\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    " \n",
    "# Get reports and metrics\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "                        \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "                              \n",
    "p,r,f1,support = precision_recall_fscore_support(y_test, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  24 | elapsed:   24.2s remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:   48.8s remaining:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  24 | elapsed:   51.0s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  24 | elapsed:   53.4s remaining:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  24 | elapsed:  1.2min remaining:   34.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  24 | elapsed:  1.2min remaining:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  1.4min remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.899\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.001\n",
      "\ttfidf__use_idf: False\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      1.00      0.94     20422\n",
      "        True       0.92      0.12      0.21      2756\n",
      "\n",
      "    accuracy                           0.89     23178\n",
      "   macro avg       0.91      0.56      0.58     23178\n",
      "weighted avg       0.90      0.89      0.86     23178\n",
      "\n",
      "Confusion Matrix\n",
      "[[20394    28]\n",
      " [ 2421   335]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for SGDClassifier Take 2\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = SGDClassifier(loss='log')\n",
    "\n",
    "# vectorizerW = TfidfVectorizer(min_df=1, analyzer='word', stop_words=None, lowercase=True)\n",
    "# vectorizerC = TfidfVectorizer(min_df=1, analyzer='char', stop_words=None, lowercase=True)\n",
    "# combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "\n",
    "#clf = Pipeline([(\"vect\", vectorizer), (\"clf\", classifier)])\n",
    "\n",
    "\n",
    "# countVect = CountVectorizer(max_features = 10000, ngram_range = (1,2))\n",
    "# tfidfTrans = TfidfTransformer(norm = 'l2')\n",
    "# combined_features = FeatureUnion([(\"vect\", countVect), (\"tfidf\", tfidfTrans)])\n",
    "# pipeline = Pipeline([('features', combined_features), \n",
    "#                 (\"classifier\", classifier)])\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "                ('tfidf', TfidfTransformer(norm = 'l2')), \n",
    "                (\"clf\", classifier)])\n",
    "\n",
    "param_grid = dict(\n",
    "#                 features__word__max_features=[2000,4000],\n",
    "#                 features__char__max_features=[2000,4000],\n",
    "#                 features__word__min_df=[2,3],\n",
    "#                 features__char__min_df=[2,3],\n",
    "#                 features__word__ngram_range=[(1,2), (1,3)],\n",
    "#                 features__char__ngram_range=[(3,3), (3,4), (4,4)],\n",
    "                # features__word__stop_words=[stop_words, 'english', None],\n",
    "#                 features__word__lowercase=[True],\n",
    "#                 features__char__lowercase=[True],\n",
    "                vect__ngram_range=[(1,2),(1,3)],\n",
    "                tfidf__use_idf=[True,False],\n",
    "                clf__alpha=[1e-2, 1e-3]\n",
    "                )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10, n_jobs=-1)\n",
    "if __name__ == \"__main__\":\n",
    "# fit on TRAINING data\n",
    "    grid_search.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                            #print(\"\\n\")\n",
    "    \n",
    "# Run the grid_search transforms+prediction with best parameters on test data\n",
    "y_pred = grid_search.predict(test_comments['comment'])\n",
    "y_test = test_comments['attack']\n",
    "\n",
    " \n",
    "# Get reports and metrics\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "                        \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "                              \n",
    "p,r,f1,support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was completed using Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What are the text cleaning methods you tried? What are the ones you have included in the final code?**\n",
    "I tried removing all the special characters, for example: %^&()<>?/\\|}{~:, etc... I replaced them with a space and then broke down the sentences into arrays of words, then concatenated these words into new sentences without any leading or trailing space. This method was included in the final code. \n",
    "\n",
    "One thing I tried but did not work out was to remove all the stop words. However, I decided to leave it for the hyperparameters tuning part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What are the features you considered using? What features did you use in the final code?** \n",
    "Originally I tried using a bag of words representation with word and character n-grams, individually, and together. I also tried 2 different extraction methods (CountVectorizer + TfidfTransformer, or TfidfVectorizer). I didn't use non-word features such as \"year\" because I did not think it was useful for clarifying attacks. The \"year\" of the comments did not offer anything that could help us with the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. What optimizations did you add in your code, if any?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?**\n",
    "These are the classifiers that I used:\n",
    "1. LinearSVC\n",
    "1. LinearDiscriminantAnalysis\n",
    "1. MLPClassifier\n",
    "1. RandomForestClassifier\n",
    "1. MultinomialNB\n",
    "1. SGDClassifier\n",
    "\n",
    "Best ML method is SGDClassifier as it returned the highest score among all methods: 0.944, even though it's still lower than that of the strawman code. LinearSVC and MLPClassifier took forever to run while LinearDiscriminantAnalysis made the kernel die. The best scores for MLPClassifier, RandomForestClassifier, and MultinomialNB were 0.930, 0.903, 0.936 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. What hyper-parameter tuning did you do, and by how many percentage points did your accuracy go up?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. What did you learn from the different metrics? Did you try cross-validation?**\n",
    "To get the best model pre-hypermeter-tuning, I tried KFold cross-validation with number of folds = 5. This means splitting the data and target into 5 equal parts used for training and testing.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. What are your best final Result Metrics? By how much is it better than the strawman figure? Which model gave you this performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h. What is the most interesting thing you learned from doing the report?**\n",
    "The most interesting thing is trying different models to figure out which one gave me the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. What was the hardest thing to do?**\n",
    "The hardest thing was trying to piece all the information together. I did not feel prepared even after watching the 2 lectures on sklearn and spending a significant amount of time looking through tutorials. However, I started to get a feel of it eventually after trying out different models. One other thing was some models took really long to run. I had to stop the kernel from running because one model took almost 2 hours to run. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
