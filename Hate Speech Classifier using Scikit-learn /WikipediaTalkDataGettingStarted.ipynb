{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was completed using Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "## download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "## download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTIONS + CODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. What are the text cleaning methods you tried? What are the ones you have included in the final code?**\n",
    "\n",
    "I tried removing all the special characters, for example: %^&()<>?/\\|}{~:, etc... I replaced them with a space and then broke down the sentences into arrays of words, then concatenated these words into new sentences without any leading or trailing spaces. This method was included in the final code. \n",
    "\n",
    "One thing I tried but did not work out was to remove all the stop words. However, I decided to leave it for the hyperparameters tuning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "# remove special characters\n",
    "specialChar = '[=@_!#$%^&*()<>?/\\|}{~:`]'\n",
    "for string in specialChar:\n",
    "    comments['comment'] = comments['comment'].apply(lambda x: x.replace(string, \" \"))\n",
    "\n",
    "# split the untrimmed sentences into arrays of words to remove the space\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.split())\n",
    "\n",
    "# join the words to create new sentences and trim trailing, leading spaces \n",
    "s = ' '\n",
    "comments['comment'] = comments['comment'].apply(lambda x: s.join(x))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279                           Iraq is not good USA is bad\n",
       "2702703    fuck off you little asshole. If you want to ta...\n",
       "4632658          i have a dick, its bigger than yours hahaha\n",
       "6545332    renault you sad little bpy for driving a renau...\n",
       "6545351    renault you sad little bo for driving a renaul...\n",
       "7977970    34, 30 Nov 2004 UTC Because you like to accuse...\n",
       "8359431    You are not worth the effort. You are arguing ...\n",
       "8724028    Yes, complain to your rabbi and then go shoot ...\n",
       "8845700                     i am using the sandbox, ass wipe\n",
       "8845736    GOD DAMN GOD DAMN it fuckers, i am using the G...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. What are the features you considered using? What features did you use in the final code?** \n",
    "\n",
    "Originally I tried using a bag of words representation with word and character n-grams, individually, and together. I also tried 2 different extraction methods (CountVectorizer + TfidfTransformer, or TfidfVectorizer). I didn't use non-word features such as \"year\" because I did not think it was useful for classifying attacks. The \"year\" of the comments did not offer anything that could help us with the data training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. What optimizations did you add in your code, if any?**\n",
    "\n",
    "Optimizations most occured during text cleaning process. I experimented with different number of folds for KFold in order to determine which one was efficient in both time and space complexity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d. What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?**\n",
    "\n",
    "These are the classifiers that I used:\n",
    "1. LinearSVC\n",
    "1. LinearDiscriminantAnalysis\n",
    "1. MLPClassifier\n",
    "1. RandomForestClassifier\n",
    "1. MultinomialNB\n",
    "1. SGDClassifier\n",
    "\n",
    "Best ML method is SGDClassifier (**code below**) as it returned the highest ROC AUC score among all methods: 0.949, even though it's still lower than that of the strawman code. LinearSVC and MLPClassifier took forever to run while LinearDiscriminantAnalysis made the kernel die. The best scores for MLPClassifier, RandomForestClassifier, and MultinomialNB were 0.930, 0.903, 0.942 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier Model With KFold 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.946\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20387\n",
      "        True       0.96      0.34      0.50      2786\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.94      0.67      0.73     23173\n",
      "weighted avg       0.92      0.92      0.90     23173\n",
      "\n",
      "[[20348    39]\n",
      " [ 1838   948]]\n",
      "Test ROC AUC: 0.943\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20458\n",
      "        True       0.96      0.34      0.51      2715\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.94      0.67      0.73     23173\n",
      "weighted avg       0.92      0.92      0.90     23173\n",
      "\n",
      "[[20421    37]\n",
      " [ 1779   936]]\n",
      "Test ROC AUC: 0.949\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20465\n",
      "        True       0.96      0.34      0.51      2708\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.94      0.67      0.73     23173\n",
      "weighted avg       0.93      0.92      0.90     23173\n",
      "\n",
      "[[20429    36]\n",
      " [ 1776   932]]\n",
      "Test ROC AUC: 0.944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20486\n",
      "        True       0.96      0.34      0.50      2687\n",
      "\n",
      "    accuracy                           0.92     23173\n",
      "   macro avg       0.94      0.67      0.73     23173\n",
      "weighted avg       0.93      0.92      0.90     23173\n",
      "\n",
      "[[20453    33]\n",
      " [ 1780   907]]\n",
      "Test ROC AUC: 0.938\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      1.00      0.96     20478\n",
      "        True       0.96      0.34      0.50      2694\n",
      "\n",
      "    accuracy                           0.92     23172\n",
      "   macro avg       0.94      0.67      0.73     23172\n",
      "weighted avg       0.92      0.92      0.90     23172\n",
      "\n",
      "[[20438    40]\n",
      " [ 1774   920]]\n"
     ]
    }
   ],
   "source": [
    "# SGDClassifier Model With KFold \n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "              'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "              'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "              'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "              'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "              'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "    classifier = SGDClassifier(loss='log')\n",
    "\n",
    "\n",
    "    clf = Pipeline([('vect', CountVectorizer(analyzer='word', stop_words=stop_words)),\n",
    "                    ('tfidf', TfidfTransformer()), \n",
    "                    (\"clf\", classifier)])\n",
    "        \n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e. What hyper-parameter tuning did you do, and by how many percentage points did your accuracy go up?**\n",
    "\n",
    "I played around with the parameters for the Classifier (alpha), and for the CountVectorizer (ngram_range, max_features, min_df, 'word' analyzer, stop_words), and for the TfidfTransformer (norm, smooth_idf, sublinear_idf, use_idf). Most parameters consisted of 1 or 2 inputs because I did not want to substantially increase the time complexity. \n",
    "\n",
    "For the list of stop words, I used the list from the following source: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/.\n",
    "\n",
    "My accuracy remains the same as the strawman figure (0.94) but some other metrics improved, which I will talk about in part g. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f. What did you learn from the different metrics? Did you try cross-validation?**\n",
    "\n",
    "To get the best model pre-hyperparameter-tuning, I tried KFold cross-validation with number of folds = 5. This means splitting the data and target into 5 train/test sets. This was a reasonable number because it guaranteed intensive testing/training while not sacrificing time complexity. When tuning hyperparameters, I used GridSearchCV with 4-fold cross validation for moderately good results with faster runtime.     \n",
    "\n",
    "The metrics returned varied results in each fold. I picked the one with the highest ROC AUC score for each model and then from these models, determined which model was the highest. I also checked to see if accuracy or precision or recall improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g. What are your best final Result Metrics? By how much is it better than the strawman figure? Which model gave you this performance?**\n",
    "\n",
    "Best final Result Metrics is below. The model that gave the best score was SGDClassifier with GridSearchCV 4-fold. The best score (0.941) is lower than strawman's ROC AUC score (0.957). However, there were some improvements in other metrics.\n",
    "\n",
    "Accuracy remains the same 0.94. Precision score for \"False\" comments (when a comment is classified as non-attack) went up by 0.01. For \"True\" comments (when a comment is classified as attack), recall went up by 0.05 and f-1 score went from 0.69 to 0.72. \n",
    "\n",
    "As a result, in the confusion matrix, the final result did slightly better for \"True\" comments and slightly worse for \"False\" comments than the strawman figure.\n",
    "\n",
    "Overall, the LinearRegression model used in the strawman is still the best one. Originally its ROC AUC score was 0.957. However, I tried it with KFold (split 5) and managed to bring its score higher to 0.963. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL RESULTS: Hyperparameters tuning for SGDClassifier, CV=4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 256 candidates, totalling 1024 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   22.5s\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 21.2min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 29.0min\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed: 30.8min\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed: 32.3min\n",
      "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed: 34.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 35.9min\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed: 41.7min\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed: 43.6min\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters tuning for SGDClassifier, CV=4 \n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = SGDClassifier(loss='log')\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "              'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "              'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "              'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "              'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "              'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer(norm = 'l2')), \n",
    "                (\"clf\", classifier)])\n",
    "\n",
    "param_grid = dict(\n",
    "                vect__analyzer=['word'],\n",
    "                vect__max_features=[10000,20000],\n",
    "                vect__ngram_range=[(1,2),(1,3)],\n",
    "                vect__stop_words=[stop_words],\n",
    "                vect__min_df=[1,2],\n",
    "                tfidf__norm=['l2', None],\n",
    "                tfidf__smooth_idf=[True,False],\n",
    "                tfidf__sublinear_tf=[True,False],\n",
    "                tfidf__use_idf=[True,False],\n",
    "                clf__alpha=[1e-2, 1e-3],\n",
    "                )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, cv=4,param_grid=param_grid, verbose=10, n_jobs=-1)\n",
    "if __name__ == \"__main__\":\n",
    "    # fit on TRAINING data\n",
    "    grid_search.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                            \n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# Run the grid_search transforms+prediction with best parameters on test data\n",
    "y_pred = grid_search.predict(test_comments['comment'])\n",
    "y_test = test_comments['attack']\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], grid_search.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# Get reports and metrics\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "                        \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "                              \n",
    "p,r,f1,support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original strawman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.97     20422\n",
      "        True       0.92      0.55      0.69      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.93      0.77      0.83     23178\n",
      "weighted avg       0.94      0.94      0.93     23178\n",
      "\n",
      "[[20281   141]\n",
      " [ 1234  1522]]\n"
     ]
    }
   ],
   "source": [
    "# Original Strawman\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "trueVals = test_comments['attack']\n",
    "predictedVals = clf.predict(test_comments['comment'])\n",
    "print(classification_report(trueVals, predictedVals))\n",
    "print(confusion_matrix(trueVals,predictedVals))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strawman with KFold 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20387\n",
      "        True       0.89      0.59      0.71      2786\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.92      0.79      0.84     23173\n",
      "weighted avg       0.94      0.94      0.94     23173\n",
      "\n",
      "[[20183   204]\n",
      " [ 1147  1639]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20458\n",
      "        True       0.90      0.59      0.71      2715\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.93      0.79      0.84     23173\n",
      "weighted avg       0.94      0.94      0.94     23173\n",
      "\n",
      "[[20287   171]\n",
      " [ 1118  1597]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20465\n",
      "        True       0.89      0.60      0.72      2708\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.92      0.80      0.84     23173\n",
      "weighted avg       0.94      0.94      0.94     23173\n",
      "\n",
      "[[20261   204]\n",
      " [ 1080  1628]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20486\n",
      "        True       0.89      0.58      0.70      2687\n",
      "\n",
      "    accuracy                           0.94     23173\n",
      "   macro avg       0.92      0.78      0.84     23173\n",
      "weighted avg       0.94      0.94      0.94     23173\n",
      "\n",
      "[[20301   185]\n",
      " [ 1134  1553]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC AUC: 0.953\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20478\n",
      "        True       0.88      0.58      0.70      2694\n",
      "\n",
      "    accuracy                           0.94     23172\n",
      "   macro avg       0.91      0.78      0.83     23172\n",
      "weighted avg       0.94      0.94      0.94     23172\n",
      "\n",
      "[[20259   219]\n",
      " [ 1139  1555]]\n"
     ]
    }
   ],
   "source": [
    "# Strawman with KFold 5-fold\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "              'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "              'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "              'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "              'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "              'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "kfold = KFold(5,True,1)\n",
    "\n",
    "X = np.array(comments['comment'])\n",
    "y = np.array(comments['attack'])\n",
    "\n",
    "for train_comments, test_comments in kfold.split(X):\n",
    "    X_train, X_test = X[train_comments], X[test_comments]\n",
    "    y_train, y_test = y[train_comments], y[test_comments]\n",
    "\n",
    "\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    vectorizerW = TfidfVectorizer(analyzer='word', stop_words=stop_words)\n",
    "    vectorizerC = TfidfVectorizer(analyzer='char')\n",
    "    combined_features = FeatureUnion([(\"word\", vectorizerW), (\"char\", vectorizerC)])\n",
    "    clf = Pipeline([(\"features\", combined_features), (\"clf\", classifier)])\n",
    "\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    trueVals = y_test\n",
    "    predictedVals = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(trueVals, predictedVals))\n",
    "    print(confusion_matrix(trueVals,predictedVals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h. What is the most interesting thing you learned from doing the report?**\n",
    "\n",
    "The most interesting thing is trying different models to figure out which one gave me the best results. They were fun to play with and it was also enjoyable tweaking all the models to find the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i. What was the hardest thing to do?**\n",
    "\n",
    "The hardest thing was trying to piece all the information together. I did not feel prepared even after watching the 2 lectures on sklearn and spending a significant amount of time looking through tutorials. However, I started to get a feel of it eventually after trying out different models. One other thing was some models took really long to run. I had to stop the kernel from running because one model took almost 2 hours to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  2.9min\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 22.8min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed: 27.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.941\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\ttfidf__norm: None\n",
      "\ttfidf__smooth_idf: False\n",
      "\ttfidf__sublinear_tf: True\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_features: 10000\n",
      "\tvect__ngram_range: (1, 3)\n",
      "\tvect__stop_words: ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
      "Test ROC AUC: 0.911\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.99      0.97     20422\n",
      "        True       0.88      0.59      0.71      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.92      0.79      0.84     23178\n",
      "weighted avg       0.94      0.94      0.94     23178\n",
      "\n",
      "Confusion Matrix\n",
      "[[20207   215]\n",
      " [ 1117  1639]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for SGDClassifier Take 2\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = SGDClassifier(loss='log')\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "              'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "              'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "              'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "              'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "              'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer(norm = 'l2')), \n",
    "                (\"clf\", classifier)])\n",
    "\n",
    "param_grid = dict(\n",
    "                vect__analyzer=['word'],\n",
    "                vect__max_features=[5000,10000],\n",
    "                vect__ngram_range=[(1,2),(1,3)],\n",
    "                vect__stop_words=[stop_words],\n",
    "                #vect__min_df=[1,2],\n",
    "                tfidf__norm=['l1', 'l2', None],\n",
    "                tfidf__smooth_idf=[True,False],\n",
    "                tfidf__sublinear_tf=[True,False],\n",
    "                tfidf__use_idf=[True,False],\n",
    "                clf__alpha=[1e-2, 1e-3],\n",
    "                #clf__class_weight=['balanced', None]\n",
    "                )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, cv=3,param_grid=param_grid, verbose=10, n_jobs=-1)\n",
    "if __name__ == \"__main__\":\n",
    "# fit on TRAINING data\n",
    "    grid_search.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                            #print(\"\\n\")\n",
    "    sys.stdout.flush()\n",
    "# Run the grid_search transforms+prediction with best parameters on test data\n",
    "y_pred = grid_search.predict(test_comments['comment'])\n",
    "y_test = test_comments['attack']\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], grid_search.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# Get reports and metrics\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "                        \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "                              \n",
    "p,r,f1,support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1024 candidates, totalling 2048 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done  89 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  7.4min\n",
      "/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 121 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 157 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 218 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 241 tasks      | elapsed: 17.8min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=-1)]: Done 289 tasks      | elapsed: 21.5min\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=-1)]: Done 341 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 368 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 397 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=-1)]: Done 457 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed: 36.0min\n",
      "[Parallel(n_jobs=-1)]: Done 521 tasks      | elapsed: 38.5min\n",
      "[Parallel(n_jobs=-1)]: Done 554 tasks      | elapsed: 40.8min\n",
      "[Parallel(n_jobs=-1)]: Done 589 tasks      | elapsed: 43.3min\n",
      "[Parallel(n_jobs=-1)]: Done 624 tasks      | elapsed: 45.8min\n",
      "[Parallel(n_jobs=-1)]: Done 661 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=-1)]: Done 698 tasks      | elapsed: 52.2min\n",
      "[Parallel(n_jobs=-1)]: Done 737 tasks      | elapsed: 55.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed: 57.9min\n",
      "[Parallel(n_jobs=-1)]: Done 817 tasks      | elapsed: 61.2min\n",
      "[Parallel(n_jobs=-1)]: Done 858 tasks      | elapsed: 65.0min\n",
      "[Parallel(n_jobs=-1)]: Done 901 tasks      | elapsed: 68.1min\n",
      "[Parallel(n_jobs=-1)]: Done 944 tasks      | elapsed: 70.9min\n",
      "[Parallel(n_jobs=-1)]: Done 989 tasks      | elapsed: 74.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1034 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1081 tasks      | elapsed: 80.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed: 83.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1177 tasks      | elapsed: 87.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed: 90.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1277 tasks      | elapsed: 95.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1328 tasks      | elapsed: 98.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1381 tasks      | elapsed: 102.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1434 tasks      | elapsed: 107.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1489 tasks      | elapsed: 111.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1544 tasks      | elapsed: 115.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1601 tasks      | elapsed: 119.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1658 tasks      | elapsed: 123.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1717 tasks      | elapsed: 128.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 132.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1837 tasks      | elapsed: 136.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1898 tasks      | elapsed: 141.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1961 tasks      | elapsed: 146.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2024 tasks      | elapsed: 151.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2048 out of 2048 | elapsed: 153.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.939\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__class_weight: 'balanced'\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__smooth_idf: True\n",
      "\ttfidf__sublinear_tf: False\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_features: 20000\n",
      "\tvect__min_df: 3\n",
      "\tvect__ngram_range: (1, 2)\n",
      "\tvect__stop_words: ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
      "Test ROC AUC: 0.940\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.80      0.88     20422\n",
      "        True       0.38      0.91      0.53      2756\n",
      "\n",
      "    accuracy                           0.81     23178\n",
      "   macro avg       0.68      0.86      0.71     23178\n",
      "weighted avg       0.91      0.81      0.84     23178\n",
      "\n",
      "Confusion Matrix\n",
      "[[16282  4140]\n",
      " [  238  2518]]\n"
     ]
    }
   ],
   "source": [
    "# Take 4\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "classifier = SGDClassifier(loss='log')\n",
    "\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out',\n",
    "              'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', \n",
    "              'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don',\n",
    "              'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', \n",
    "              'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', \n",
    "              'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', \n",
    "              'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being',\n",
    "              'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "\n",
    "pipeline = Pipeline([('vect', TfidfVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()), \n",
    "                (\"clf\", classifier)])\n",
    "\n",
    "param_grid = dict(\n",
    "                vect__analyzer=['word','char'],\n",
    "                vect__max_features=[10000,20000],\n",
    "                vect__ngram_range=[(1,2),(1,3)],\n",
    "                vect__stop_words=[stop_words],\n",
    "                tfidf__norm=['l2', None],\n",
    "                tfidf__smooth_idf=[True,False],\n",
    "                tfidf__sublinear_tf=[True,False],\n",
    "                tfidf__use_idf=[True,False],\n",
    "                clf__alpha=[1e-2, 1e-3],\n",
    "                #vect__ngram_range=[(1,2),(1,3)],\n",
    "                #vect__stop_words=[stop_words],\n",
    "                vect__min_df=[3,4],\n",
    "                #tfidf__norm=['l1', 'l2', None],\n",
    "                #tfidf__smooth_idf=[True,False],\n",
    "                #tfidf__sublinear_tf=[True,False],\n",
    "                #tfidf__use_idf=[True,False],\n",
    "                #clf__alpha=[1e-2, 1e-3],\n",
    "                clf__class_weight=['balanced',None]\n",
    "                )\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, cv=2,param_grid=param_grid, verbose=10, n_jobs=-1)\n",
    "if __name__ == \"__main__\":\n",
    "# fit on TRAINING data\n",
    "    grid_search.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(param_grid.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "                            #print(\"\\n\")\n",
    "    sys.stdout.flush()\n",
    "# Run the grid_search transforms+prediction with best parameters on test data\n",
    "y_pred = grid_search.predict(test_comments['comment'])\n",
    "y_test = test_comments['attack']\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], grid_search.predict_proba(test_comments['comment'])[:, 1])\n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "\n",
    "# Get reports and metrics\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "                        \n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "                              \n",
    "p,r,f1,support = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
